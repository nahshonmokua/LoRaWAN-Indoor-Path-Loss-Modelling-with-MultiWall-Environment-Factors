{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0731ebb-947f-42f6-b67f-746cac323711",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Courier New', Courier, monospace; font-size: 40px; font-weight: bold; color: blue;  text-align: center;\">\n",
    "  LoRaWAN Path Loss Measurements in an Indoor Setting: DATA QUERYING from InfluxdB\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4a656-6186-4b7e-974d-34f92e9ccdb9",
   "metadata": {},
   "source": [
    "# Notebook Summary\n",
    "This Notebook fetches sensor data from InfluxDB in batches, processes it, and updates a CSV file with combined measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3499d8ae-78a4-479a-a36d-963313455252",
   "metadata": {},
   "source": [
    "### 1. Libraries and Packages\n",
    "Imports OS, dotenv, Pandas, InfluxDBClient, time, and pathlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f08d57-f8ec-4201-84e0-cb2920960d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries/Packages Used:\n",
    "import os                            # For accessing environment variables\n",
    "from dotenv import load_dotenv       # To load environment variables from the .env file\n",
    "import pandas as pd                  # For data manipulation and handling timezones\n",
    "from influxdb import InfluxDBClient  # To interact with InfluxDB (a time-series database)\n",
    "import time                          # For sleep function between batches\n",
    "import pathlib                       # For checking if file exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430bcd87-68e8-421d-9bad-9851ae483843",
   "metadata": {},
   "source": [
    "### 2. Load Environment Variables\n",
    "Loads .env variables and retrieves InfluxDB host, port, and database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a3bf8e6-1e2a-44c9-87ea-6a0472a3e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the variables\n",
    "host = os.getenv('INFLUXDB_HOST')\n",
    "port = int(os.getenv('INFLUXDB_PORT'))  # port is an integer\n",
    "database = os.getenv('INFLUXDB_DATABASE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9c282-24b8-43f3-b8e5-0fcc5b0468c0",
   "metadata": {},
   "source": [
    "### 3. Function: fetch_data\n",
    "Fetches sensor data between start and end times by converting Europe/Berlin to UTC (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0363bdd-9d2a-458a-9af2-3640aff64b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(start_time, end_time):\n",
    "    \"\"\"\n",
    "    Fetch sensor data from InfluxDB between specified start and end times.\n",
    "\n",
    "    Converts input times from Europe/Berlin to UTC for querying, then back to Europe/Berlin for use.\n",
    "    Returns data as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Initialize the InfluxDB client\n",
    "    client = InfluxDBClient(host=host, port=port)\n",
    "    client.switch_database(database)\n",
    "\n",
    "    # Convert input times (Europe/Berlin) to UTC for the query\n",
    "    start_time_utc = pd.to_datetime(start_time).tz_localize('Europe/Berlin').tz_convert('UTC').strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    end_time_utc = pd.to_datetime(end_time).tz_localize('Europe/Berlin').tz_convert('UTC').strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "    # Querying with the provided start_time and end_time in UTC\n",
    "    # '>' for start_time to exclude the last fetched timestamp\n",
    "    query = f\"SELECT * FROM sensor_data WHERE time > '{start_time_utc}' AND time <= '{end_time_utc}'\"\n",
    "\n",
    "    result = client.query(query)\n",
    "    df = pd.DataFrame(list(result.get_points()))\n",
    "\n",
    "    if not df.empty:\n",
    "        # Convert 'time' column to datetime with utc=True\n",
    "        df['time'] = pd.to_datetime(df['time'], utc=True, format='ISO8601').dt.tz_convert('Europe/Berlin')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9dbc7-bd4d-4cc0-aff0-d885caac392e",
   "metadata": {},
   "source": [
    "### 4. Function: fetch_data_in_batches\n",
    "Fetches sensor data in 10-day batches with a 2-minute pause between queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f22f09-bbbb-4e90-973b-41a9d8d1d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_in_batches(start_time, end_time):\n",
    "    \"\"\"\n",
    "    Fetch sensor data in batches of 10 days with a 2-minute  break between each batch.\n",
    "    \"\"\"\n",
    "    # Convert start_time to a datetime object\n",
    "    start_time = pd.to_datetime(start_time)\n",
    "\n",
    "    # Check if start_time is timezone-naive; if so, localize it\n",
    "    if start_time.tzinfo is None:\n",
    "        start_time = start_time.tz_localize('Europe/Berlin')\n",
    "    else:\n",
    "        start_time = start_time.tz_convert('Europe/Berlin')\n",
    "\n",
    "    # Check if end_time is timezone-naive; if so, localize it\n",
    "    end_time = pd.to_datetime(end_time)\n",
    "    if end_time.tzinfo is None:\n",
    "        end_time = end_time.tz_localize('Europe/Berlin')\n",
    "    else:\n",
    "        end_time = end_time.tz_convert('Europe/Berlin')\n",
    "\n",
    "    # Initialize a list to store DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    current_start = start_time\n",
    "    delta = pd.Timedelta(days=10)\n",
    "\n",
    "    while current_start < end_time:\n",
    "        current_end = min(current_start + delta, end_time)\n",
    "\n",
    "        print(f\"Fetching data from {current_start} to {current_end}\")\n",
    "\n",
    "        # Fetch data for the current interval\n",
    "        df = fetch_data(current_start.strftime('%Y-%m-%d %H:%M:%S'), current_end.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "        if not df.empty:\n",
    "            df_list.append(df)\n",
    "\n",
    "        # Sleep for 2 minutes between queries\n",
    "        if current_end < end_time:\n",
    "            print(\"Sleeping for 2 minutes...\")\n",
    "            time.sleep(120)\n",
    "\n",
    "        # Move to the next interval\n",
    "        current_start = current_end\n",
    "\n",
    "    # Combine all DataFrames\n",
    "    if df_list:\n",
    "        batch_combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    else:\n",
    "        batch_combined_df = pd.DataFrame()\n",
    "\n",
    "    return batch_combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8320d2ed-c46e-4231-93f2-9854dcfda6a3",
   "metadata": {},
   "source": [
    "###  5. Main Data Fetching & CSV Update\n",
    "Checks for an existing CSV, resumes from the last timestamp if available, fetches new data, cleans it (drops all-NaN columns), aligns, concatenates, and saves the updated CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32ee144f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resuming data fetching from 2025-03-29 02:01:54.081417+01:00.\n",
      "Fetching data from 2025-03-29 02:01:54.081417+01:00 to 2025-03-29 12:05:49.416993+01:00\n",
      "The following columns are entirely NaN in the newly fetched data and will be dropped:\n",
      "\n",
      " - uplink_message_rx_metadata_0_frequency_offset\n",
      " - uplink_message_rx_metadata_1_channel_index\n",
      " - uplink_message_rx_metadata_1_channel_rssi\n",
      " - uplink_message_rx_metadata_1_frequency_offset\n",
      " - uplink_message_rx_metadata_1_gateway_ids_eui\n",
      " - uplink_message_rx_metadata_1_gateway_ids_gateway_id\n",
      " - uplink_message_rx_metadata_1_received_at\n",
      " - uplink_message_rx_metadata_1_rssi\n",
      " - uplink_message_rx_metadata_1_snr\n",
      " - uplink_message_rx_metadata_1_time\n",
      " - uplink_message_rx_metadata_1_timestamp\n",
      " - uplink_message_rx_metadata_1_uplink_token\n",
      " - uplink_message_rx_metadata_2_channel_index\n",
      " - uplink_message_rx_metadata_2_channel_rssi\n",
      " - uplink_message_rx_metadata_2_frequency_offset\n",
      " - uplink_message_rx_metadata_2_gateway_ids_eui\n",
      " - uplink_message_rx_metadata_2_gateway_ids_gateway_id\n",
      " - uplink_message_rx_metadata_2_received_at\n",
      " - uplink_message_rx_metadata_2_rssi\n",
      " - uplink_message_rx_metadata_2_snr\n",
      " - uplink_message_rx_metadata_2_time\n",
      " - uplink_message_rx_metadata_2_timestamp\n",
      " - uplink_message_rx_metadata_2_uplink_token\n",
      "\n",
      "Data fetching completed and saved to '../all_data_files/unsorted_combined_measurements_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# File path for the combined data CSV\n",
    "csv_file_path = '../all_data_files/unsorted_combined_measurements_data.csv'\n",
    "\n",
    "# Starting campaign time\n",
    "initial_start_time = '2024-09-26 13:00:00'  # Berlin time\n",
    "\n",
    "# Check if the CSV file exists\n",
    "file_exists = pathlib.Path(csv_file_path).exists()\n",
    "\n",
    "if file_exists:\n",
    "    # Read existing data\n",
    "    combined_df = pd.read_csv(csv_file_path, parse_dates=['time'], low_memory=False)\n",
    "    if not combined_df.empty:\n",
    "        # Get the last timestamp\n",
    "        last_timestamp = combined_df['time'].max()\n",
    "        # Start from the last timestamp\n",
    "        start_time = last_timestamp\n",
    "        print(f\"\\nResuming data fetching from {start_time}.\")\n",
    "    else:\n",
    "        combined_df = pd.DataFrame()\n",
    "        start_time = initial_start_time\n",
    "        print(f\"The existing CSV file is empty. Starting data fetching from {start_time}.\")\n",
    "else:\n",
    "    combined_df = pd.DataFrame()\n",
    "    start_time = initial_start_time\n",
    "    print(f\"No existing CSV file found. Starting data fetching from {start_time}.\")\n",
    "\n",
    "# End time is the current time (Berlin)\n",
    "end_time = pd.Timestamp.now(tz='Europe/Berlin')\n",
    "\n",
    "# Fetch new data in batches (your custom function)\n",
    "new_data_df = fetch_data_in_batches(start_time, end_time)\n",
    "\n",
    "# Combine with existing data if we actually got something\n",
    "if not new_data_df.empty:\n",
    "    # -----------------------------------------------------------\n",
    "    # 1. Identify columns in new_data_df that are entirely NaN\n",
    "    all_na_cols = new_data_df.columns[new_data_df.isna().all()].tolist()\n",
    "    \n",
    "    if all_na_cols:\n",
    "        print(\"The following columns are entirely NaN in the newly fetched data and will be dropped:\\n\")\n",
    "        # Print each column name as an individual list item\n",
    "        for col in all_na_cols:\n",
    "            print(f\" - {col}\")\n",
    "    \n",
    "    # 2. Drop those columns from new_data_df\n",
    "    new_data_df.drop(columns=all_na_cols, inplace=True)\n",
    "\n",
    "    # 3. Align columns and concatenate with combined_df\n",
    "    if not combined_df.empty:\n",
    "        # Determine the union of columns\n",
    "        all_cols = sorted(set(combined_df.columns) | set(new_data_df.columns))\n",
    "        \n",
    "        # Reindex both DataFrames to ensure all columns align\n",
    "        combined_df = combined_df.reindex(columns=all_cols)\n",
    "        new_data_df = new_data_df.reindex(columns=all_cols)\n",
    "        \n",
    "        # Concatenate the DataFrames\n",
    "        combined_df = pd.concat([combined_df, new_data_df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = new_data_df.copy()\n",
    "\n",
    "    # Save the combined DataFrame\n",
    "    try:\n",
    "        combined_df.to_csv(csv_file_path, index=False)\n",
    "        print(f\"\\nData fetching completed and saved to '{csv_file_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFailed to save data to CSV: {e}\")\n",
    "else:\n",
    "    print(\"No new data fetched.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_models_env)",
   "language": "python",
   "name": "ml_models_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
